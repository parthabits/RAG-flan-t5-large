{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "OXIX92exmKIk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8gtUrhbjQ3b",
        "outputId": "74909616-26c1-4fdb-8f5d-f325a7774d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q python-docx sentence-transformers faiss-cpu transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface Token"
      ],
      "metadata": {
        "id": "x-0SlF8xmRG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "Lul68J3dkNDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Implementation for word document"
      ],
      "metadata": {
        "id": "t7VZlxQXmUmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"### Code using DOCX\"\"\"\n",
        "\n",
        "# Step 1: Extracting text from DOCX\n",
        "def extract_text_from_docx(docx_path):\n",
        "    '''\n",
        "    Purpose: Extracting text from docx file\n",
        "\n",
        "    docx_path: path to the docx file\n",
        "    '''\n",
        "    doc = docx.Document(docx_path)\n",
        "    text = []\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text.append(paragraph.text)  # We are storing text per paragraph\n",
        "    return text\n",
        "\n",
        "# Step 2: Chunking text by sequence length (2500 tokens)\n",
        "def chunk_text_by_length(text_list, chunk_size=2500):\n",
        "    '''\n",
        "    Purpose: Chunking text by sequence length (2500 tokens)\n",
        "\n",
        "    text_list: list of text\n",
        "    chunk_size: 2500 (default)\n",
        "    '''\n",
        "    chunks = []\n",
        "    for page_text in text_list:\n",
        "        tokens = page_text.split()  # Tokenizing by whitespace\n",
        "        for i in range(0, len(tokens), chunk_size):\n",
        "            chunk = \" \".join(tokens[i:i + chunk_size])  # Creating chunks of 2500 tokens\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Embedding the text chunks\n",
        "def embed_chunks(chunks, model):\n",
        "    '''\n",
        "    Purpose: Embedding the text chunks\n",
        "\n",
        "    chunks: list of chunks\n",
        "    model: embedding model\n",
        "    '''\n",
        "    embeddings = model.encode(chunks, batch_size=8, convert_to_numpy=True)  # Embedding chunks in batches\n",
        "    return embeddings\n",
        "\n",
        "# Step 4: Create FAISS index\n",
        "def create_faiss_index(embeddings):\n",
        "    '''\n",
        "    Purpose: Creating FAISS index\n",
        "\n",
        "    embeddings: embeddings\n",
        "    '''\n",
        "    dimension = embeddings.shape[1]  # Getting the dimension of the embeddings\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
        "    index.add(embeddings)  # Adding embeddings to the index\n",
        "    return index\n",
        "\n",
        "# Step 5: Performing vector search\n",
        "def vector_search(query, model, faiss_index, chunks, top_k=5):\n",
        "    '''\n",
        "    Purpose: Performing vector search\n",
        "\n",
        "    query: search query\n",
        "    model: embedding model\n",
        "    faiss_index: faiss index\n",
        "    chunks: list of chunks\n",
        "    top_k: number of top elements to retrieve\n",
        "    '''\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)  # Encoding the query\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)  # Performing FAISS search\n",
        "    relevant_chunks = [chunks[idx] for idx in indices[0]]  # Getting the most relevant chunks\n",
        "    return relevant_chunks\n",
        "\n",
        "# Step 6: Generating an answer using the relevant chunks\n",
        "def generate_answer(query, context, model, tokenizer):\n",
        "    '''\n",
        "    Purpose: Generating an answer using the relevant chunks\n",
        "\n",
        "    query: the user query\n",
        "    context: the context retrieved from vector search\n",
        "    model: the language model\n",
        "    tokenizer: the tokenizer for the model\n",
        "    '''\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer in brief:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    outputs = model.generate(input_ids, max_length=200, num_beams=3, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Main function to perform RAG\n",
        "def run_rag(docx_path, query, embedding_model, faiss_index, chunks, llm_model, tokenizer):\n",
        "    '''\n",
        "    Purpose: Main function to perform RAG\n",
        "\n",
        "    docx_path: path to the docx file\n",
        "    query: user query\n",
        "    embedding_model: embedding model\n",
        "    faiss_index: faiss index\n",
        "    chunks: list of text chunks\n",
        "    llm_model: language model\n",
        "    tokenizer: tokenizer for the language model\n",
        "    '''\n",
        "    relevant_chunks = vector_search(query, embedding_model, faiss_index, chunks, top_k=5)\n",
        "    context = \" \".join(relevant_chunks)  # Combining relevant chunks as context\n",
        "    answer = generate_answer(query, context, llm_model, tokenizer)\n",
        "    return answer.strip()\n",
        "\n",
        "# Step 7: Running the process\n",
        "def main():\n",
        "    # Loading models\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight embedding model\n",
        "    llm_model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-large').to('cuda')  # Loading on GPU\n",
        "    tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-large')\n",
        "\n",
        "    # Loading DOCX and extracting text\n",
        "    docx_path = '66. GST Smart Guide (1).docx'  # DOCX file path\n",
        "    text_list = extract_text_from_docx(docx_path)\n",
        "\n",
        "    # Chunking text based on sequence length\n",
        "    chunks = chunk_text_by_length(text_list, chunk_size=500)\n",
        "\n",
        "    # Embedding chunks\n",
        "    embeddings = embed_chunks(chunks, embedding_model)\n",
        "\n",
        "    # Creating FAISS index\n",
        "    faiss_index = create_faiss_index(embeddings)\n",
        "\n",
        "    # Asking a question and retrieving an answer\n",
        "    query = \"Taxpayer is not allowed to use ITC in excess of what percentage of output tax liability?\"\n",
        "    answer = run_rag(docx_path, query, embedding_model, faiss_index, chunks, llm_model, tokenizer)\n",
        "\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKhZFmy3jzJx",
        "outputId": "73bc19a0-17e6-4280-ca18-ce85cb505015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXLQiX-YkSed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}