{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a9EskGAPWM6o"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "a9EskGAPWM6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q PyPDF2 sentence-transformers faiss-cpu transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViWyXu-6-Psl",
        "outputId": "94bd3f44-6f99-4ac6-9908-2afb1767ce9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface Token"
      ],
      "metadata": {
        "id": "aUAvpm3VWSIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "xYL3MgDX-4RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "s2Ta1ItQWZGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IzLBzKzcWdMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code while using PDF"
      ],
      "metadata": {
        "id": "rvBmdX-_WlNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extracting text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    '''\n",
        "    Purpose: Extracting text from pdf\n",
        "\n",
        "    pdf_path: pdf path\n",
        "    '''\n",
        "    text = []\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text.append(page.extract_text())  # We are storing text per page\n",
        "    return text\n",
        "\n",
        "# Step 2: Chunking text by sequence length (500 tokens)\n",
        "def chunk_text_by_length(text_list, chunk_size=500):\n",
        "    '''\n",
        "    Purpose: Chunking text by sequence length (500 tokens)\n",
        "\n",
        "    text_list: text list\n",
        "    chunk_size: 500 (default)\n",
        "    '''\n",
        "    chunks = []\n",
        "    for page_text in text_list:\n",
        "        tokens = page_text.split()  # Tokenizing by whitespace\n",
        "        for i in range(0, len(tokens), chunk_size):\n",
        "            chunk = \" \".join(tokens[i:i + chunk_size])  # Creating chunks of 500 tokens\n",
        "            # Small note: Linewise chunking would possibly give sub-optimal answers in case we are looking for multi line answer.\n",
        "            #             Obviously by conjugating top k would possibly help in getting the answer even with linewise chunking, but that would be too tedious.\n",
        "            #             Pagewise chunking would cause cude outofmemory in compute suboptimal devices like colab environment\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Embedding the text chunks\n",
        "def embed_chunks(chunks, model):\n",
        "    '''\n",
        "    Purpose: Embedding the text chunks\n",
        "\n",
        "    chunks: chunks\n",
        "    model: embedding model\n",
        "    '''\n",
        "    embeddings = model.encode(chunks, batch_size=8, convert_to_numpy=True)  # Embedding chunks in batches\n",
        "    return embeddings\n",
        "\n",
        "# Step 4: Create FAISS index\n",
        "def create_faiss_index(embeddings):\n",
        "    '''\n",
        "    Purpose: Creating FAISS index\n",
        "\n",
        "    embeddings: embeddings\n",
        "    '''\n",
        "    dimension = embeddings.shape[1]  # Getting the dimension of the embeddings\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
        "    index.add(embeddings)  # Adding embeddings to the index\n",
        "    return index\n",
        "\n",
        "# Step 5: Performing vector search\n",
        "def vector_search(query, model, faiss_index, chunks, top_k=5):\n",
        "    '''\n",
        "    Purpose: Performing vector search\n",
        "\n",
        "    query: query\n",
        "    model: embedding model\n",
        "    faiss_index: faiss_index\n",
        "    chunks: chunks\n",
        "    top_k: top k elements to return from vector search\n",
        "    '''\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)  # Encoding the query\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)  # Performing FAISS search\n",
        "    relevant_chunks = [chunks[idx] for idx in indices[0]]  # Getting the most relevant chunks\n",
        "    return relevant_chunks\n",
        "\n",
        "# Step 6: Generating an answer using the relevant chunks\n",
        "def generate_answer(query, context, model, tokenizer):\n",
        "    '''\n",
        "    Purpose: Generating an answer using the relevant chunks\n",
        "\n",
        "    query: query\n",
        "    context: context,\n",
        "    model: model,\n",
        "    tokenizer: tokenizer\n",
        "    '''\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer in brief:\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    outputs = model.generate(input_ids, max_length=200, num_beams=3, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Main function to perform RAG\n",
        "def run_rag(pdf_path, query, embedding_model, faiss_index, chunks, llm_model, tokenizer):\n",
        "    '''\n",
        "    Purpose: Main function to perform RAG\n",
        "\n",
        "    pdf_path: pdf_path,\n",
        "    query: query,\n",
        "    embedding_model: embedding_model,\n",
        "    faiss_index: faiss_index,\n",
        "    chunks: chunks,\n",
        "    llm_model: llm_model,\n",
        "    tokenizer: tokenizer\n",
        "    '''\n",
        "    relevant_chunks = vector_search(query, embedding_model, faiss_index, chunks, top_k=5)\n",
        "    context = \" \".join(relevant_chunks)  # Combining relevant chunks as context\n",
        "    answer = generate_answer(query, context, llm_model, tokenizer)\n",
        "    return answer.strip()\n",
        "\n",
        "# Step 7: Running the process\n",
        "def main():\n",
        "    # Loading models\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight embedding model\n",
        "    llm_model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-large').to('cuda')  # Loading on GPU\n",
        "    tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-large')\n",
        "\n",
        "    # Loading PDF and extracting text\n",
        "    pdf_path = 'G S T Smart Guide.pdf'  # PDF file path\n",
        "    text_list = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Chunking text based on sequence length\n",
        "    chunks = chunk_text_by_length(text_list, chunk_size=500)\n",
        "\n",
        "    # Embedding chunks\n",
        "    embeddings = embed_chunks(chunks, embedding_model)\n",
        "\n",
        "    # Creating FAISS index\n",
        "    faiss_index = create_faiss_index(embeddings)\n",
        "\n",
        "    # Asking a question and retrieving an answer\n",
        "    query = \"Who are eligible to take Input Tax Credit?\"\n",
        "    answer = run_rag(pdf_path, query, embedding_model, faiss_index, chunks, llm_model, tokenizer)\n",
        "\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "plcwPtkWI_dL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ca5aad-94cb-4bcb-a72b-af45a67e1762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3168 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: every registered person shall, subject to such conditions and restrictions prescribed under Section 49, be entitled to take credit of input tax charged on any supply of goods or services or both which are used or intended to be used in the course or furtherance of his business. 8. Who are not eligible to take Input Tax Credit? A registered person working under composition scheme even when received goods or services are used in furtherance of his business. A non-resident taxable person on receipt of goods and services except on goods imported by him. 9. Input Tax Credit not available on the goods and services 10. Other restrictions of Input Tax Credit 113 11. Clarification on various issues pertaining to GST as under: 115 12. Eligibility & conditions for taking Input Tax Credit 115 13. ITC appears in restricted category, the same cannot be availed by the registered person 116 14. Manner of distribution of credit by Input Service Distributor 119 15. Documents’\n"
          ]
        }
      ]
    }
  ]
}